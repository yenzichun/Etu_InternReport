sqoop:
	
#從PostgreSQL export table schema 到 Hive
sqoop create-hive-table --connect jdbc:postgresql://etu-master/hive --username hive -P --table nyse_dividends --hive-table etu203.nyse_dividends
#從PostgreSQL export 資料到 Hive
sqoop import --connect jdbc:postgresql://etu-master/hive --username hive -P --table nyse_dividends --hive-import --hive-table etu203.nyse_dividends -m 1

$ psql -U hive -d hive -W -h `hostname -f`
 sqoop create-hive-table --connect jdbc:postgresql://etu-master/hive --username hive -P --table nyse_daily --hive-table etu203.nyse_daily 
 sqoop import --connect jdbc:postgresql://etu-master/hive --username hive -P --table nyse_daily --hive-import --hive-table etu203.nyse_daily -m 1 

--connect <jdbc-uri> 		Specify JDBC connect string
--driver <class-name> 		Manually sprcify JDBC driver class to use
--help 						Print usage instructions
--P							Read password from console
--password <password>		Set authentication passeord
--username <username>		Set authentication username
--verbose					Print more information while working
--table <table-name>		Table to read
--target-dir <dir>			HDFS destination dir
--hive-table				Specify target table name in Hive
--hive-import				Specify sqoop target to Hive
-m	<n>						Use n map tasks to import in parallel




#Q1:請透過  Sqoop  將  PostgreSQL  上位於  hive database  下的  nyse_dividends table 
#匯到  Hive  的 etu203 database  之下

	#從PostgreSQL export table schema 到 Hive
	$	sqoop create-hive-table --connect jdbc:postgresql://etu-master/hive --username hive -P --table nyse_dividends --hive-table etu203.nyse_dividends
	#從PostgreSQL export 資料到 Hive
	$	sqoop import --connect jdbc:postgresql://etu-master/hive --username hive -P --table nyse_dividends --hive-import --hive-table etu203.nyse_dividends -m 1
